# Netflix Azure Data Engineering End-to-End Pipeline using Azure Databricks

## Project Architecture
<img width="974" height="668" alt="image" src="https://github.com/user-attachments/assets/97220445-67f2-4aa4-af4d-ba8050a23112" />


<img width="995" height="278" alt="image" src="https://github.com/user-attachments/assets/efe15eb8-8201-4b23-b928-548685784391" />
<img width="913" height="248" alt="image" src="https://github.com/user-attachments/assets/8a51f3e1-7fbb-4f22-be63-cc1d4477f544" />
<img width="930" height="315" alt="image" src="https://github.com/user-attachments/assets/004b2801-b872-42b0-9692-18aa507560e6" />
<img width="802" height="556" alt="image" src="https://github.com/user-attachments/assets/f0a02417-b0f8-46a5-a6b2-17d00df57628" />



## ğŸ“Œ Project Overview

This project demonstrates an **end-to-end data engineering pipeline** built using **Azure Databricks**, **Delta Lake**, and **Delta Live Tables (DLT)**.

The pipeline ingests raw Netflix data, processes it using the **Medallion Architecture (Bronze â†’ Silver â†’ Gold)**, enforces **data quality rules**, and follows modern **data engineering best practices**.

This project is designed to simulate a **real-world, production-grade data platform**.

---

## ğŸ” Azure Data Factory â€“ Incremental Ingestion

Azure Data Factory (ADF) is used for **orchestration** and **incremental ingestion** of data from APIs dynamically.

### Key Features
- GitHub-integrated ADF pipelines
- Web activity to fetch metadata from GitHub API
- Validation activity to verify incoming data
- ForEach activity to process multiple files dynamically
- Incremental Copy activity to ADLS Gen2

---

## ğŸ”— Git Integration Strategy (ADF)

Azure Data Factory follows a Git-based workflow for source control and deployment.

- **`main` branch**
  - Used for ADF pipeline authoring
  - Contains logical pipeline definitions created via ADF UI

- **`adf_publish` branch**
  - Auto-generated by ADF during Publish
  - Contains ARM templates used for deployment

This setup follows **Azure-recommended CI/CD practices**.

---

## âš™ï¸ Databricks & Delta Live Tables (DLT)

Databricks handles large-scale data processing and transformations.

### Features
- Auto Loader for incremental ingestion
- Delta Lake for ACID-compliant storage
- Delta Live Tables for managed pipelines
- Built-in data quality checks using `EXPECT` rules
- Streaming and batch processing

---

## ğŸ§± Medallion Architecture

- **Bronze**: Raw ingested data from source
- **Silver**: Cleaned, enriched, and standardized data
- **Gold**: Business-ready curated tables for analytics

---

## ğŸ“ Project Structure

```
netflix-data-engineering-databricks/
â”œâ”€â”€ 1_AutoLoader.ipynb
â”œâ”€â”€ 2_silver.ipynb
â”œâ”€â”€ 3_lookup.ipynb
â”œâ”€â”€ 4_silver.ipynb
â”œâ”€â”€ 5_LookUpNotebook.ipynb
â”œâ”€â”€ 6_GetDayNumber.ipynb
â”œâ”€â”€ 7_DLT_Notebook.ipynb
â”œâ”€â”€ dataset/
â”œâ”€â”€ pipeline/
â”œâ”€â”€ linkedService/
â”œâ”€â”€ factory/
â”œâ”€â”€ publish_config.json
â””â”€â”€ README.md
```



## ğŸ“˜ Notebook Descriptions

### 1ï¸âƒ£ Auto Loader (1_AutoLoader.ipynb)
- Ingests Netflix CSV files from ADLS Gen2
- Uses **Databricks Auto Loader**
- Supports **incremental file ingestion**
- Stores raw data in **Bronze Delta tables**

---

### 2ï¸âƒ£ Silver Layer Transformations (2_silver.ipynb & 4_silver.ipynb)
- Cleans raw data
- Handles null values
- Parses and standardizes date formats
- Splits duration fields (minutes / seasons)
- Applies schema normalization

---

### 3ï¸âƒ£ Lookup Processing (3_lookup.ipynb & 5_LookUpNotebook.ipynb)
- Creates lookup datasets
- Supports downstream joins and enrichments
- Improves query performance and readability

---

### 4ï¸âƒ£ Utility Notebook (6_GetDayNumber.ipynb)
- Derives weekday/day-number logic
- Demonstrates parameter passing using Databricks widgets
- Shows job task value handling

---

### 5ï¸âƒ£ Gold Layer using Delta Live Tables (7_DLT_Notebook.ipynb)
- Creates **Gold tables** using **DLT**
- Reads from Silver Delta tables
- Applies **data quality rules using DLT expectations**
- Publishes curated, analytics-ready datasets

---

## ğŸ¥‡ Gold Tables Created

- `gold_netflix_titles`
- `gold_netflix_cast`
- `gold_netflix_directors`
- `gold_netflix_categories`
- `gold_netflix_countries`

These tables are:
- Fully managed by **Delta Live Tables**
- Registered in **Unity Catalog**
- Stored as **Delta tables** in the configured storage location

---

## âœ… Data Quality & Governance

- Implemented **DLT expectations** to validate data (e.g., non-null IDs)
- Invalid records are automatically dropped
- Centralized metadata and access control using **Unity Catalog**
- Built-in lineage and monitoring via Databricks UI

---

## ğŸ“Š Pipeline Execution

- Pipeline executed using **Databricks Delta Live Tables**
- Supports **streaming execution**
- Monitoring available via:
  - Pipeline graph
  - Event logs
  - Data quality metrics

---

## ğŸš€ Key Learnings

- Designing end-to-end data pipelines on Azure
- Incremental ingestion with Auto Loader
- Streaming vs batch processing
- Implementing Medallion Architecture
- Building governed datasets with Unity Catalog
- Enforcing data quality using Delta Live Tables
- Version control integration with GitHub

---

## ğŸš€ Technologies Used

- Azure Data Factory
- Azure Databricks
- Delta Lake
- Delta Live Tables (DLT)
- Azure Data Lake Storage Gen2
- GitHub (Version Control)
- Python & SQL


---

## ğŸ‘¤ Author

**Rohan Dubey**  
Aspiring Data Engineer  
GitHub: https://github.com/RohanDubey45  

---

â­ If you find this project useful, feel free to star the repository!
